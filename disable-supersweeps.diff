xen,pod: Disable PoD supersweeps

Testing has shown that doing sweeps for superpages slows down
boot significantly, but does not result in a significantly higher
number of superpages after boot.  Disable it completely.

Signed-off-by: George Dunlap <george.dunlap@eu.citrix.com>

diff -r 5467e154c8e6 xen/arch/x86/mm/p2m.c
--- a/xen/arch/x86/mm/p2m.c	Wed Oct 05 16:37:11 2011 +0100
+++ b/xen/arch/x86/mm/p2m.c	Wed Oct 05 16:37:17 2011 +0100
@@ -912,7 +912,6 @@ p2m_pod_dump_data(struct p2m_domain *p2m
            p2m->pod.entry_count, p2m->pod.count, p2m->pod.superpage_count);
 }
 
-
 /* Search for all-zero superpages to be reclaimed as superpages for the
  * PoD cache. Must be called w/ p2m lock held, page_alloc lock not held. */
 static int
@@ -1160,34 +1159,6 @@ p2m_pod_zero_check(struct p2m_domain *p2
 }
 
 #define POD_SWEEP_LIMIT 1024
-static void
-p2m_pod_emergency_sweep_super(struct p2m_domain *p2m)
-{
-    unsigned long i, start, limit;
-
-    if ( p2m->pod.reclaim_super == 0 )
-    {
-        p2m->pod.reclaim_super = (p2m->pod.max_guest>>9)<<9;
-        p2m->pod.reclaim_super -= SUPERPAGE_PAGES;
-    }
-    
-    start = p2m->pod.reclaim_super;
-    limit = (start > POD_SWEEP_LIMIT) ? (start - POD_SWEEP_LIMIT) : 0;
-
-    for ( i=p2m->pod.reclaim_super ; i > 0 ; i -= SUPERPAGE_PAGES )
-    {
-        p2m_pod_zero_check_superpage(p2m, i);
-        /* Stop if we're past our limit and we have found *something*.
-         *
-         * NB that this is a zero-sum game; we're increasing our cache size
-         * by increasing our 'debt'.  Since we hold the p2m lock,
-         * (entry_count - count) must remain the same. */
-        if ( !page_list_empty(&p2m->pod.super) &&  i < limit )
-            break;
-    }
-
-    p2m->pod.reclaim_super = i ? i - SUPERPAGE_PAGES : 0;
-}
 
 /* When populating a new superpage, look at recently populated superpages
  * hoping that they've been zeroed.  This will snap up zeroed pages as soon as 
@@ -1294,19 +1265,16 @@ p2m_pod_demand_populate(struct p2m_domai
     /* Once we've ballooned down enough that we can fill the remaining
      * PoD entries from the cache, don't sweep even if the particular
      * list we want to use is empty: that can lead to thrashing zero pages 
-     * through the cache for no good reason.  */
-    if ( p2m->pod.entry_count > p2m->pod.count )
-    {
-
-        /* If we're low, start a sweep */
-        if ( order == 9 && page_list_empty(&p2m->pod.super) )
-            p2m_pod_emergency_sweep_super(p2m);
-
-        if ( page_list_empty(&p2m->pod.single) &&
-             ( ( order == 0 )
-               || (order == 9 && page_list_empty(&p2m->pod.super) ) ) )
-            p2m_pod_emergency_sweep(p2m);
-    }
+     * through the cache for no good reason.
+     *
+     * Sweep for 4k pages if:
+     *  - 4k list is empty and we're looking for a 4k page
+     *  - We're looking for a 2MiB page but both lists are empty */
+    if ( p2m->pod.entry_count > p2m->pod.count
+         && page_list_empty(&p2m->pod.single) 
+         && ( ( order == 0 ) 
+              || (order == 9 && page_list_empty(&p2m->pod.super) ) ) )
+        p2m_pod_emergency_sweep(p2m);
 
     /* Keep track of the highest gfn demand-populated by a guest fault */
     if ( q == p2m_guest && gfn > p2m->pod.max_guest )
